\section{Preprocessing}
\tikzstyle{diabox}=[minimum height=1.4cm,shape=rectangle,align=center, draw, font=\small]
\begin{figure*}[t]
    \begin{tikzpicture}
        \node[diabox] at (0.00,0) (parsing) {Email\\Parsing};
        \node[diabox] at (2.2,0,0) (textproc) {Text Processing\\Techniques};
        \node[diabox] at (4.6,0) (domain) {Domain\\Extraction};
        \node[diabox] at (6.6,0) (stemming) {Stemming};
        \node[diabox] at (8.65, 0) (weight) {Feature\\Weighting};
        \node[diabox] at (10.6,0)  (fselection) {Feature\\Selection};
        \node[diabox] at (12.7, 0) (training) {Classifier\\Training};
        \node[diabox] at (15, 0) (classify) {Test Set\\Classification};
        \draw[red,dashed] (-1,-1) rectangle ++(12.6,2);
        \draw[red] (0.1,1.4) node {Preprocessing};
        \draw[blue,dashed] (7.6,-1.2) rectangle ++(8.7,2.4);
        \draw[blue] (14.9,1.6) node {Cross Validation};
        \draw [->] (parsing.east) -- (textproc.west);
        \draw [->] (textproc.east) -- (domain.west);
        \draw [->] (domain.east) -- (stemming.west);
        \draw [->] (stemming.east) -- (weight.west);
        \draw [->] (weight.east) -- (fselection.west);
        \draw [->] (fselection.east) -- (training.west);
        \draw [->] (training.east) -- (classify.west);
    \end{tikzpicture}
    \caption{Steps taken during training and cross validation}
    \label{fig:steps}
\end{figure*}

The bag of words model used by our classifiers is a good way to represent the data found in the given training documents. However this feature representation is prone to very high dimensions if not handled well due to the large number of possible combinations in words. 

According to a recent survey performed by Google and Harvard, there are approximately 1,022,000 different words in the English language \cite{google2010words}. While we do not expect to find all possible words in the document corpus, a large number of them will be found along with their derivations in the form of spelling mistakes and grammatical variations (e.g.{\it they're} and {\it theyre}). Other non-English words such as HTML code, URLs and seemingly nonsensical data such as PGP keys will also be found within the documents.

Most of these words will provide no benefit to the classifier and in most cases will even harm the classifier's performance. It is therefore of utmost importance that noise in the text is filtered out and the number of word combinations is reduced to the most representative, yet minimal subset of the available words. Once this is done, steps can be taken to transform each document into a feature vector that can be understood by the classifier. 

The following steps are taken to do this:
\begin{itemize}
	\item Smart Email Parsing
	\item Simple Text Processing Techniques
	\item Domain Extraction
	\item Stemming
	\item Inverted Index Storage
	\item Feature Selection based on Document Frequency
	\item Vector Generation / Feature Weighting
\end{itemize}

Each of these steps contribute towards achieving a higher accuracy and performance in the classifier and will be described in detail in the sections below.

\subsection{Email Parsing}
As a first preprocessing step, we make use of a modified parser to extract information from each email's structure.
The provided emails will sometimes contain metadata which is very redundant w.r.t. to the actual class. 
An example of this is the occurrence of "From:" which is contained within the headers of all emails. 
The \emph{presence} of metadata in emails is in fact independent from any class label, i.e. the classifier should not use it its presence as a feature for distinguishing between classes of emails.
On the other hand the information \emph{contained} within the metadata itself can be very useful in determining the class if available, but such a feature will be left for future work due to time constraints.
Finally emails do not always consist of just text but may also have attachments encoded plain text or beautified with HTML markup. 


As a first step we strip most of the metadata from the email so that we only return the actual content of the email to the classifier.
This drastically reduces the amount of data passed to the classifier as most terms in the metadata are either common among all emails, e.g. the names of the header lines (``From'', ``Content-Type'', \dots), or are completely unique to the mail itself like the date of sending or the message-id.
From the metadata we only extract some basic information like the encoding or flags stating if the email is multi-part.

If the mail is split into multiple parts, we only extract parts which contain plain text as we cannot infer information from the \emph{base64} representation of images and other binary attachments.
These binary attachments are represented in the form of seemingly random text (to the classifier) and would only add a large number of garbled text \cite{rfc2045} to the resultant index. Such combinations of text are very unlikely to be found in other emails due to high entropy within binary data.
Another problem that is solved by handling multiple parts is the recursive parsing of attached emails. For each embedded email, we strip down each inner email to just their content as we have just described in this section.

Furthermore not all text within the email is encoded using same encoding as the declared by the email itself. For example the main text could be encoded as Base64 which would not reveal the actual words when just splitting the mail into words by separating at each space.
Another typical encoding is quoted-printable which transforms non-ASCII characters into another format (so called escape characters) but leaves all ASCII characters intact.
Handling these multiple encodings in the correct fashion transforms meaningless, long character sequences into words that are present in other emails and useful for the classifier.

As a final parsing step we strip down as much HTML content as possible from the emails' content in order to prevent HTML tags from entering our word index.
This markup is not displayed to the user and does not carry any useful content for the parser or classifier to make use of.
For extracting the text from the HTML document we are using the Java Library jsoup \cite{jsoup} which provides error-resistant parsing of even non-standard HTML.

Although "smart" email parsing did not cause any significant increase in the quality scores of the classifier, it did manage to reduce the dimensionality from 143820 down to 94903 words.
As a consequence of this lower dimensionality the runtime decreases from 16 to just 10 minutes.
In conclusion email parsing is responsible for stripping down a lot of irrelevant or redundant dependencies from the input data set without the cost of any loss in quality.

\subsection{Simple Text Processing Techniques}

A number of simple processing techniques are used to conflate strings being returned by the parser so that variations of the same word are mapped to the same feature. Although the email parser is built in such a way as to strip out HTML content where possible, a number of artefacts could still linger in the data which could cause noise and incorrectly distinguish variations of the same word. Because it is not the parser's job to perform this form of filtering, a separate step is taken to perform simple text processing tasks before passing them on the later pre-processing stages.

All incoming words that are composed purely of symbols (i.e. no numbers or letters) are simply discarded as they are most commonly noise in the data that do not represent anything in the corpus. Without this step, the number of features used by the classifier would grow substantially as a large number of "rubbish" symbols are included as part of the feature set. Additionally, all remaining words will have all symbols removed as this prevents variations in words like \emph{they're} and \emph{theyre} from being treated as different features.

As a second step, all valid incoming words are simply reduced to lower-case format. The same word in different cases should not be distinguished from one another during classification so it is important that e.g. \emph{Bristol} is considered the same as \emph{bristol}.

Finally, variations in number representations (e.g. 4,000 and 4000) are detected using regular expressions and conflated to a single representative feature ``9999''. We do this because allowing all possible number combinations a unique feature each will increase the dimensionality substantially and rarely contribute to classification performance. Doing this also provides some assurance that we do not overfit our classifier to the training set with specific numeric combinations.

Using the techniques described above we were able to reduce the number of  combinations from 94,903 words to just 56149. This provides a substantial decrease in dimensions and provides a very notable improvement to memory usage, training speed and classification speed. 

\subsection{Domain Extraction}

Due to the fact that Emails are a product of the web, it is very common to find items such as Uniform Resource Locators (URLs) and email addresses within the their contents. When left unprocessed, it is hard to group these items into corresponding features due to the additional data included within them.

What we really care about is the domain each URL and email address contains. If we find two URLs which point to the same domain such as \emph{www.cs.bris.ac.uk/maths} and \emph{www.cs.bris.ac.uk/eng}, then it would make sense to group these two items together as one feature - i.e. coming from the domain \emph{cs.bris.ac.uk}. This will prove useful in being able to identify URLs which are associated with spam and those which are well known and credible ham domains.

To perform this step, we use a regular expression parser to detect items that are URLs and email addresses. Detected items are put through a \emph{Domain Extraction} function which is able to strip out the username and '@' symbol in the case of emails (e.g. \emph{johndoe@example.com} would become \emph{example.com}) and remove paths and protocol declarations in the case of URLs (e.g. \emph{https://www.example.com/myimage.jpg} would become \emph{example.com} once again). Conflating both email address domains and web URL domains makes sense because they represent the same source of information.


\subsection{Stemming}
Most words in the English language are derived from a \emph{morphological root} word that contains no prefixes or suffixes and conveys a very similar meaning to its derivation. A simple example of this is \emph{subscriber} and \emph{subscription} with their morphological root \emph{subscribe}. 

If we can reduce all incoming words into their root form, we would be able to substantially reduce the number of dimensions for our model while also ensuring that words representing the same underlying feature are stored under the same value.

Unfortunately, such a task is quite hard and would probably require the creation of a very large lookup table for each word in the English language along with its root. This is due to the fact that the English language is not a formal language and hence does not follow a strict set of rules. 

We could however, take an approximation of the described process and instead derive the \emph{stem} of each word. Like the morphological root, the stem is a representation of a words underlying meaning. However the stem does not guarantee to be a correct English word or generate the right root as its aim is simply to map variations of the same word to the same to item. 

For our Spam Filter implementation, we made use of the Porter Stemmer algorithm \cite{porter1980}, which in the authors words is ``a process for removing the commoner morphological and inflexional endings from words in English''. In simpler terms, it is capable of removing known suffixes from words passed to it. The Porter Stemmer algorithm is available as open source code under the BSD License and is available in multiple languages, including Java which is made use of in our implementation.

Using the same examples shown before, passing \emph{subscriber} and \emph{subscribe} to the Porter Stemmer would reduce the words to the stem \emph{subscrib}. On the other hand however, the word \emph{subscription} will be wrongly mapped to a different stem - \emph{subscript}. The latter is an example of where the approximation fails to produce the correct result, however in general most words passed to the algorithm have shown to produce favourable results.

In terms of the Spam Filter implementation, using Porter Stemming on the given set of training emails reduced the number of words from 24813 words to 18932 \textit{stems} (both after text pre-processing). This is a substantial reduction in the number of dimensions and plays a crucial role in ensuring that the classifier is able to train with the given documents in a short amount of time and without requiring large amounts of memory.

\subsection{Inverted Index Storage}

After a word has gone through the stages of simple text processing, domain extraction and stemming - it is stored along with the name of its document of origin in a fast inverted index structure using a Hashed List. The Hash List is built so that a given word is an index that is able to quickly retrieve or update information about its metadata in constant $\mathcal O(1)$ time. Metadata included for each term includes a Hashed Map from a \textit{document name} to the frequency of the term in the document.

As more words are added over time, the inverted index will accumulate a list of frequencies for each word in several documents. Once all words have been passed, information about the corpus such as each word's total frequency and its frequency within each document can easily be calculated from the underlying metadata. The information stored is especially important for the next two stages that will occur - Feature Selection and Feature Weighting.

\subsection{Feature selection}
Although we have substantially reduced the number of features through the use of various text processing techniques, we still have a substantial number of features for the classifier to train with. Interestingly, the frequency of occurrence for each word in the text can be grouped into one of 3 general categories of \textit{very common}, \textit{common} and \textit{rare}. 

Very frequent elements in the text and commonly referred to as stopwords in linguistic morphology. These are words which are very frequently found in English texts (Some good examples are \emph{the}, \emph{and}, \emph{it} etc..) and usually do not provide any particular indication about the category of the document's class. On the other hand, very rare elements in a text are usually attributed to spelling mistakes and other artefacts such as IDs (PGP keys being the most common in emails). In between these two categories we have words not too commonly found, but not rare enough to constitute as errors either.

A corpus of documents that shows this pattern is said to obey \emph{Zipf's law} and thus follow what is called a \emph{Zipfian Distribution}. It so happens that most English documents follow this law and so does our corpus. This pattern occurs in English texts mainly due to a phenomenon pointed out by Zipf in his paper which he calls  \emph{the principle of least effort} \cite{zipf1949}.

The frequency of occurrence of each term in the given training set is shown in Figure \ref{zipfian} mapped in descending order. Some words on the graph are specified on the x-axis to provide an sample at each portion of the graph (Note that these features are stems rather than words due to our previous Stemming technique). It should be immediately noted how many different rare words are found within the corpus with low frequency by the long "tail" shown in the graph. On the other hand one should also notice how very frequent words such as \emph{the} do not have many combinations when compared to the infrequent words.

What we really want to keep is the words which are in between these two categories as these are the most representative of the text. This area can be considered somewhat of a \emph{"Goldilocks Zone"} as its just about right for giving us the right information. To extract this area and remove the excess, we use a technique called \textit{feature selection} to trim out all features from the index with a document frequency less than \(\alpha * no.\:documents\) and more than \(\beta * no.\:documents\), where \(\alpha\) and \(\beta\) are values in the range $[0,1]$.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{zipfian.png}
    \caption{Zipfian Distribution exhibited in the Training Corpus}
    \label{zipfian}
\end{figure}

Unfortunately, choosing ideal \(\alpha \) and \(\beta\) is a somewhat tedious process of trial and error.
Finding the right values ensures that the classifier does not overfit on features that are only available in the training process or accidentally strips out features which are useful to the classifier.
In Section \ref{sec:featurethresholds}, we will try to optimise our selected thresholds by the empirical evaluation of the performance of the classifier on our data set.

\subsection{Vector Generation / Feature Weighting}

Now that we have filtered out all unnecessary features and determined which features should be kept for classification, we need to convert email documents into data that is understandable by the Classifier. More specifically, we need to convert each email into a corresponding feature vector using the features we have just identified. In this report, we have attempted two ways of converting frequency information found in emails into corresponding weights. 

The first and most basic attempt uses \emph{Frequency Weighting} which is simply assigning each feature in a vector the frequency of the corresponding term in the email. Although seemingly simple as an approach, it has proven to be an effective representation of each email in the corpus during the writing of this report.

The second approach attempted was \emph{Term Frequency Inverse Document Frequency} also commonly known as \emph{tfidf} \cite{tfidf1973}. Tfidf is a commonly used metric in natural language processing that portrays the importance of a word in a document with respect to the rest of the corpus. It is technique often used by Search Engines and has proven many times to be a reliable way of representing the significance of a term within a document. The equation for calculating the weight for a term $i$ in a document $j$ is given as:

\begin{align*}
	\textrm{tfidf}_{ij} = \frac{f_{ij}}{\textrm{maxf}} * \log\left(\frac{N}{n_i}\right)
\end{align*}

Where $N$ represents the number of documents, $n_i$ represents the number of times the term i appears in a document and \emph{maxf} represents the highest frequency in the document corpus.

Interestingly, after testing these two approaches, it was immediately noted that \emph{tfidf} gave significantly worse results than \emph{frequency weighting}. While this was initially a surprise, It was noted that this behaviour could possibly be occurring due to use of a multinomial Naive Bayes model rather than Bernoulli model. Further investigation as to why this is the case could be a possible area for future work on our implementation. 

We shall assume the use of Frequency Weighting for the rest of the report due to its superior performance. Note that an implementation of the tfidf weighting scheme is still included as part of this reports implementation but is left disabled in favour of frequency weighting.

\subsection{Optimising Feature Thresholds}
\label{sec:featurethresholds}

After all preprocessing steps have been performed, the total number of dimensions has now been brought down to 43437 different words.
Although this is less than a third of the initial dimensionality, it is slowing down the training of the classifier as well as the classification of the samples.
A high number of features can also have a negative impact on quality of the classification results as the classifier may learn to concentrate on words that are rare and only appear in the training set while leaving out features that could separate the two classes very well.

To increase the quality and the runtime, we need to remove infrequent and very frequent words. However we still need to keep semi-frequent features without having a negative effect on the classification quality.
As an exhaustive search of all possible feature combination is infeasible, we will try to empirically find the two thresholds $\alpha$ and $\beta$.
We do this by setting one of the thresholds to a fixed value while iterating over possible values for the other threshold.

\input{upperbound-plot.tex}

To compare the impact on the behaviour of the classification of both classes \emph{Spam} and \emph{Ham} we are using the measures \emph{Precision}, \emph{Recall} and \emph{Negative Recall} as they are less dependent of the class distribution in the test set than \emph{Accuracy}.

In Figure \ref{p:upperbound}, we have set $\alpha = 0$ to have all lower frequent words in the training and iterating over $\beta$ to empirically determine which high frequent words can be removed without an effect on the cluster quality.
As seen in the graph, for a value of $\beta$ larger than 0.19 all three quality measures remain constant.
In the interval between $0.05$ and $0.2$ there is a high variance of the scores, so we increased the resolution in the plot to a simulation in each 0.01 step to better view the behaviour of the classifier.
As a final choice for $\beta$ we have taken $\beta = 0.19$ as \emph{Recall} has its maximum here and decreases steep towards $\beta = 0$.
The other measures are at a very good level (near their maximum) for $\beta = 0.19$ and only increase slightly in this neighbourhood.

This choice of $\beta$ reduces the dimensionality down to 43331 by pruning 106 very frequent words.
These pruned words are mostly stopwords which do not provide any indication about the class of the mail as they occur in all kinds of text.
As only a low number of words were reduced, the runtime stays constant at six minutes.

\input{lowerprecisionplot.tex}

Figure \ref{p:lowerbound} shows the behaviour of the quality measures if we would evaluate the classification performance with $\beta=1$ to determine the best lower threshold.
As we already have chosen an upper threshold of $\beta = 0.19$, we are taking this into account in Figure \ref{p:lowerbound19}.
Using the upper threshold leads to slightly different behaviour were \emph{Precision} still decreases when increasing the threshold to a lower level.
Whereas in the situation $\beta = 1$ \emph{Recall} remained at a near constant level among all parameter choices, it decreases with the same pace as \emph{Precision}.
Overall it should be remarked that \emph{Precision} is at a much higher level for $\alpha = 0$ in Figure \ref{p:lowerbound19} than in Figure \ref{p:lowerbound} whilst \emph{Recall} is at approximately the same level in both situations.

In both situations, we get the best scores for $\alpha=0$, which is no further improvement on the selection of the upper threshold.
This means that there is a significant number of words that occur only once in the training set but which still help separate classes in the test set.
It should be noted that not all words with a frequency of one in the training set are relevant but there are enough relevant words with a frequency of one that would have a significant impact on the scores if pruned.

Although the best results were achieved using $\alpha=0$, for $\alpha=0.17$ we can still generate a classifier that produces very good results with a \emph{Precision} of $0.916$ and a \emph{Recall} of $0.944$.
As this classifier only utilises 1568 dimensions, which is about 4\% of the classifier with $\alpha=0$, it runs the cross validation cycle in less then 100 seconds.

\input{lowerbound-plot.tex}

% We are not going to use this as it is misleading
% \input{lowerbound-plot-zoom.tex}
