\section{Preprocessing}
\tikzstyle{diabox}=[minimum height=1.4cm,shape=rectangle,align=center, draw, font=\small]
\begin{figure*}[t]
    \begin{tikzpicture}
        \node[diabox] at (0.00,0) (parsing) {Email\\Parsing};
        \node[diabox] at (2.2,0,0) (textproc) {Text Processing\\Techniques};
        \node[diabox] at (4.6,0) (domain) {Domain\\Extraction};
        \node[diabox] at (6.6,0) (stemming) {Stemming};
        \node[diabox] at (8.65, 0) (weight) {Feature\\Weighting};
        \node[diabox] at (10.6,0)  (fselection) {Feature\\Selection};
        \node[diabox] at (12.7, 0) (training) {Classifier\\Training};
        \node[diabox] at (15, 0) (classify) {Test Set\\Classification};
        \draw[red,dashed] (-1,-1) rectangle ++(12.6,2);
        \draw[red] (0.1,1.4) node {Preprocessing};
        \draw[blue,dashed] (7.6,-1.2) rectangle ++(8.7,2.4);
        \draw[blue] (14.9,1.6) node {Cross Validation};
        \draw [->] (parsing.east) -- (textproc.west);
        \draw [->] (textproc.east) -- (domain.west);
        \draw [->] (domain.east) -- (stemming.west);
        \draw [->] (stemming.east) -- (weight.west);
        \draw [->] (weight.east) -- (fselection.west);
        \draw [->] (fselection.east) -- (training.west);
        \draw [->] (training.east) -- (classify.west);
    \end{tikzpicture}
    \caption{Steps taken during training and cross validation}
    \label{fig:steps}
\end{figure*}

The bag of words model used by our classifiers is a good way to represent the data found in the given training documents. However this feature representation is prone to very high dimensions if not handled well due to the large number of possible combinations in words. 

According to a recent survey performed by Google and Harvard, there are approximately 1,022,000 different words in the English language \cite{google2010words}. While we do not expect to find all possible words in the document corpus, a large number of them will be found along with their derivations in the form of spelling mistakes and grammatical variations (e.g.{\it they're} and {\it theyre}). Other non-English words such as html code, URLs and nonsensical data such as PGP keys will also be found within the documents.

Most of these words will provide no benefit to the classifier and in most cases will even harm the classifier's performance. It is therefore of utmost importance that noise in the text is filtered out and the number of word combinations is reduced to the most representative, yet minimal subset of the available words. Once this is done, steps can be taken to transform each document into a feature vector that can be understood by the classifier. 

The following steps are taken to do this:
\begin{itemize}
	\item Smart Email Parsing
	\item Simple Text Processing Techniques
	\item Domain Extraction
	\item Stemming
	\item Inverted Index Storage
	\item Feature Selection based on Document Frequency
	\item Vector Generation / Feature Weighting
\end{itemize}

Each of these steps contribute towards achieving a higher accuracy and performance in the classifier and will be described in detail in the sections below.

\subsection{Email Parsing}
As a first preprocessing step, we modify the parsing of the email structure.
The provided emails sometimes contain metadata which is very redundant w.r.t. to the number of contained words, e.g. all will have a "From" line but also if an email has metadata is independent from its class label, i.e. the classifier should not that all mails that have metadata are spam.
Furthermore mails do not always consist of just text but also have attachments or their textual content is supplied as plain text or beautified with HTML markup.

In a first step, we strip the metadata from the email so that only give the content of the mail to the classifier.
This reduces the size of data passed on to the classifier as most terms in the metadata are either common among all emails, e.g. the names of the header lines (``From'', ``Content-Type'', \dots), or are unique to the mail like the date of sending or the message-id.
From the metadata we extract some basic information like the encoding or if it is a multi-part email for further processing.

If the mail is split in multiple parts, we want to extract only the parts which contain text as we cannot infer information of the \emph{base64} representation of images or other binary attachments.
These binary attachments would only add a large number of words of a length of 76 characters \cite{rfc2045} to the index which are very uncommon in other mails as images are compressed data with a high entropy.
Another problem that is solved by adding handling multiple is the recursive parsing of attached emails that have textual content and may have multiple parts again inside so that we are now stripping down the inner mails to their actual content too.

Furthermore not all text is encoded in the same encoding as the email itself, e.g. the main text could be encoded as Base64 which would not reveal the actual words when just splitting the mail into words by separating at each space.
Another typical encoding is quoted-printable which only transforms non-ASCII characters into another format but leaves all ASCII characters intact.
Handling these encoding turns meaningless, long character sequences into words that are present in other mails.

As a final parsing step in the email handling we are stripping all HTML from the mails to remove all HTML tags from our word index.
This markup is not displayed to the user and does not carry any content that will be some sort of advertising.

Although email parsing did not make a significant impact on the quality score of the classifier, it reduced its dimensionality from 143820 down to 94903.
With this lower dimensionality the runtime decreases too from 16 minutes to ten.
In conclusion email parsing stripped a lot of irrelevant or redundant dependencies from the input data set without the cost of quality loss.

\subsection{Simple Text Processing Techniques}

A number of simple processing techniques are used to conflate incoming strings that represent the word together. Although the email parser is built in such a way as to strip out html content where possible, a number of artefacts could still linger in the data which could cause noise and incorrectly distinguish variations of the same word. Because it is not the parsers job to perform this form of filtering, a separate step is taken to perform simple text processing tasks before passing them on the later pre-processing stages.

All incoming words that are composed purely of symbols (i.e. no numbers or letters) are simply discarded as they are most commonly noise in the data that does not represent anything in the corpus. Without this step, the number of features used by the classifier would grow substantially as a large number of "rubbish" symbols are included as part of the feature set. Additionally, all remaining words will have ally symbols stripped, this prevents variations in words like {\it they're} and {\it theyre} from being treated as different features.

As a second step, all valid incoming words are simply reduced to lower-case format. The same word in different cases should not be distinguished from one another during classification so it is important that e.g. {\it Bristol} is considered the same as {\it bristol}.

Finally, variations in number representations (e.g. 4,000 and 4000) are detected using regular expressions and conflated to a single representative feature "9999". We do this because allowing all possible number combinations each a unique feature will increase the dimensionality substantially and rarely contribute to classification performance - it can most often be considered noise. Doing this also provides some assurance that we do not overfit our classifier to the training set with specific numeric combinations.

Using the techniques described above we were able to reduce the number of  combinations from 94,903 words to just 56149. This provides a substantial decrease in dimensions and provides a very notable improvement to memory usage, training speed and classification speed. 

\subsection{Domain Extraction}

Due to the fact that Emails are a product of the web, it is very common to find items such as Uniform Resource Locaters (URLs) and email addresses within the their text. When left unprocessed, it is hard to group these items into corresponding features due to the additional data included within them.

What we really care about is the domain each URL and email address contains. If we find two URLs which point to the same domain such as \emph{www.cs.bris.ac.uk/maths} and {\it www.cs.bris.ac.uk/eng}, the it would make sense to group these two items together as one feature - i.e. coming from the domain \emph{cs.bris.ac.uk}. This will prove useful in being able to identify URLs which point to malicious or spam websites and those which are well known and credible domains.

To do this, we use a regular expression parser to detect items that are URLs and email addresses. Detected items are put through a \emph{Domain Extraction} function which is able to strip out the username and '@' symbol in the case of emails (e.g. \emph{johndoe@example.com} would become \emph{example.com}) and remove paths and protocol declerations in the case of URLs (e.g. \emph{https://www.example.com/myimage.jpg} would become \emph{example.com} once again). Conflating both email address domains and web URL domains makes sense because they represent the same source of information.


\subsection{Stemming}
Most words in the English language are derived from a \emph{morphological root} word that contains no prefixes or suffixes and conveys a very similar meaning to its derivation. A simple example of this is \emph{subscriber} and \emph{subscription} with their morphological root \emph{subscribe}. 

If we can reduce all incoming words into their root form, we would be able to substantially reduce the number of dimensions for our model while also ensuring that words representing the same underlying feature are stored under the same value.

Unfortunately, such a task is quite hard and would probably require the creation of a very large lookup table for each word in the English language along with its root. This is due to the fact that the English language is not a formal language and hence does not follow a strict set of rules. 

We could however, take an approximation of the described process and instead derive the \emph{stem} of each word. Like the morphological root, the stem is a representation of a words underlying meaning. However the stem does not guarantee to be a correct English word or generate the right root as its aim is simply to map variations of the same word to the same to item. 

For our Spam Filter implementation, we made use of the Porter Stemmer algorithm \cite{porter1980}, which in the authors words is ``a process for removing the commoner morphological and inflexional endings from words in English''. In simpler terms, it is capable of removing known suffixes from words passed to it. The Porter Stemmer algorithm is available as open source code under the BSD License and is available in multiple languages, including Java which is made use of in our implementation.

Using the same examples shown before, passing \emph{subscriber} and \emph{subscribe} to the Porter Stemmer would reduce the words to the stem \emph{subscrib}. On the other hand however, the word \emph{subscription} will be wrongly mapped to a different stem - \emph{subscript}. The latter is an example of where the approximation fails to produce the correct result, however in general most words passed to the algorithm have shown to produce favourable results.

In terms of the Spam Filter implementation, using Porter Stemming on the given set of training emails reduced the number of words from 24813 words to 18932 \textit{stems} (both after text pre-processing). This is a substantial reduction in the number of dimensions and plays a crucial role in ensuring that the classifier is able to train with the given documents in a short amount of time and without requiring large amounts of memory.

\subsection{Inverted Index Storage}

After a word has gone through the stages of simple text processing, domain extraction and stemming - it is stored along with the name of its document of oriign in a fast inverted index structure using a Hashed List. The Hash List is built so that a given word is an index that is able to quickly retrieve or update information its metadata in instant $O(1)$ time. Metadata included for each term includes an additional Hashed Map from document names to the frequency of the term in the indexed document as well as the terms total frequency for fast access without the need to accumulate all the results.

As more words are added over time the inverted index will accumulate a list of frequencies for each word in several documents. Once all words have been passed, information about the corpus such as each word's total frequency and its frequency within each document can easily be calculated from the underlying metadata in a quick fashion. The information stored is especially important for the next two stages that will occur - Feature Selection and Feature Weighting.

\subsection{Feature selection}
Although we have substantially reduced the number of features through the use of various text processing techniques, we still have a substantial number of features for the classifier to train with. Interestingly, the frequency of occurrence for each word in the text can be grouped into 3 categories of very common, common and rare. 

Very frequent elements in the text and commonly referred to as stopwords in linguistic morphology. These are words which are very frequently found in English texts (Some good examples are \emph{the}, \emph{and}, \emph{it} etc..) and usually do not provide any particular indication about the category of the document's class. On the other hand, very rare elements in a text are usually attributed to spelling mistakes and other artefacts such as IDs (PGP keys being the most common in emails). In between these two categories we have words not too commonly found, but not rare enough to constitute as errors either.

A corpus of documents that shows this pattern is said to obey \emph{Zipf's law} and thus follow what is called a \emph{Zipfian Distribution}. It so happens that most English documents follow this law and so does our corpus. This pattern occurs in English texts mainly due to a phenomenon pointed out by Zipf in his paper which he calls  \emph{the principle of least effort} \cite{zipf1949} .

The frequency of occurrence of each term in the given training set is shown in Figure \ref{zipfian}  mapped in descending order. Some words on the graph are specified on the x-axis to provide an example at each portion of the graph (Note that these are stems due to our previous Stemming technique). It should be immediately noted how common rare words are by the long "tail" shown in the graph and one should also notice how frequent certain words such as \emph{the} are.

What we really want to keep is the words which are in between these two categories as these are the most representative of the text. This area can be considered somewhat of a \emph{"Goldilocks Zone"} as its just about right for giving us the right information. To extract this area and remove the excess, we use a technique called feature selection to trim out all features from the index with a document frequency less than \(\alpha \times no. documents\)and more than \(\beta \times no. documents\), where \(\alpha\) and \(\beta\) are values in the range [0,1].

\begin{figure}[h!]
	\label{zipfian}
    \centering
    \includegraphics[width=0.5\textwidth]{zipfian.png}
    \caption{Zipfian Distribution exhibited in the Training Corpus}
    \label{zipfian}
\end{figure}

Unfortunately, choosing ideal \(\alpha \) and \(\beta\) is a somewhat tedious process of trial and error.
Finding the right values ensure that the classifiers does not overfit on features are only available in the training process but may accidentially strip out features that contribute very good to distinguish both classes but are represented with the same frequency as term that are considered irrelevant.
In Section \ref{sec:featurethresholds}, we will try to optimise our selected thresholds by the emperical evaluation of the performance of the classifier on out data set.

\subsection{Vector Generation / Feature Weighting}

Now that we have filtered out all unnecessary features and determined which features should be kept for classification, we need to convert email documents into data that is understandable by the Classifier. More specifically, we need to convert each email into a corresponding feature vector using the features we have just identified. In this report, we have attempted two ways of converting frequency information found in emails into corresponding weights. 

The first and most basic attempt uses \emph{Frequency Weighting} which is simply assigning each feature in a vector the frequency of the corresponding term in the email. 

The second approach taken is using \emph{Term Frequency Inverse Document Frequency} also commonly known as \emph{tfidf} \cite{tfidf1973}. Tfidf is a commonly used metric in natural language processing that portrays the importance of a word in a document with respect to the rest of the corpus. It is technique often used by Search Engines and has proven many times to be a reliable way of representing the significance of a term within a document. The equation for calculating the weight for a term $i$ in a document $j$ is given as:

\begin{align*}
	\textrm{tfidf}_{ij} = \frac{f_{ij}}{\textrm{maxf}} * \log\left(\frac{N}{n_i}\right)
\end{align*}

Where $N$ represents the number of documents, $n_i$ represents the number of times the term i appears in a document and \emph{maxf} represents the highest frequency in the document corpus.

After testing these two approaches, it was immediately noted that \emph{tfidf} gave significantly worse results than \emph{frequency weighting}. While this was initially a surprise, It was noted that this behaviour could be occurring due to use of a multinomial Naive Bayes model rather than Bernoulli model. \todo{maybe some more justification on this. Not sure this is enough information to justify mentioning tfidf}. We shall assume the use of Frequency Weighting for the rest of the report for this reason.

\subsection{Optimising Feature Thresholds}
\label{sec:featurethresholds}

After all preprocessing steps the total number of dimensions is now down to 43437 different words.
Altough this is less than a third of the initial dimensionality, it is slowing down the training of the classifier as well as the classification of the samples.
A high number of features can have a negative impact on quality of the classification results too as the classifier may learn to concentrate on words that are rare and only appear in the training set while leaving out feature that could separate the two classes very well.

To increase the quality and the runtime, we want to remove infrequent and non-separating, but still freuqent features without having a negative effect on the quality.
As an exhaustive search of all possible feature combination is infeasible, we will try to find emperically the two thresholds $\alpha$ and $\beta$.
This is done by setting one of the threasholds to a fixed value while iterating over possible values for the other threshold.

\input{upperbound-plot.tex}

To compare the impact on the behaviour of the classification of both classes \emph{Spam} and \emph{Ham} we are using the measures \emph{Precision}, \emph{Recall} and \emph{Negative Recall} as they are less dependent of the class distribution in the test set than \emph{Accuracy}.

In Figure \ref{p:upperbound}, we have set $\alpha = 0$ to have all lower frequent words in the training and iterating over $\beta$ to emperically determine which high frequent words can be removed without an effect on the cluster quality.
As seen in the graph, only for a value of $\beta$ larger than 0.19 all three quality measures stay quite constant.
In the interval between $0.05$ and $0.2$ there is a high variance of the scores, so we increased the resolution in the plot to a simulation in each 0.01 step to better view the behaviour of the classifier.
As a final choice for $\beta$ we have taken $\beta = 0.19$ as \emph{Recall} has its maximum here and decreases steep towards $\beta = 0$.
The other measures are at a very good level (near their maximum) for $\beta = 0.19$ and only increase slightly in this neighbourhood.

\input{lowerprecisionplot.tex}

\input{lowerbound-plot.tex}

% We are not going to use this as it is misleading
% \input{lowerbound-plot-zoom.tex}

