\section{Naive Bayes}
As previously made in Part 1 of this project, we are using Naive Bayes as our initial classifier.
We model our the underlying distribution as a multinomial distribution based on the word frequency in each email.

The probabilites generated during training and classifying are always stored as loglikelihoods to minimize numerical errors.
Due to this low number of numerical errors, this Naive Bayes implementation is able to cope with the large number of 94903 dimensions (i.e. different word in the input data) in the initial training set.

To obtain an initial baseline performance, we run the Naive Bayes classifier on the training set using 10-fold cross validation.
This resulted in an accuracy of 0.944 and a standard deviation across all folds of 0.024 for the Naive Bayes classifier without using the underlying class distribution as a-priori probabilities.
With the usage of the underlying distribution, we get a classification accuracy of 0.9472 and a standard deviation of 0.032.

Altough the class distribution (81\% Ham, 19\% Spam) is very different to a uniform distribution, the resulting classification score are not significantly different.
With each instantiation taking 16 minutes to train and classify, the runtime does not make a difference either as the feature size stayed the same.
\todo{As using maximum-a-posteriori is the most common used Naive Bayes approach, we are going to use it in the following as our baseline which we will optimise using various preprocessing techniques.}

